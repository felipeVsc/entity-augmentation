{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3915abfd",
   "metadata": {},
   "source": [
    "# Generate popularity ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e505a6c",
   "metadata": {},
   "source": [
    "# Generate centrality ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ba47b",
   "metadata": {},
   "source": [
    "# Generate prompts for instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44057f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePromptForInstances():\n",
    "    \"\"\"\n",
    "    This function generates all the prompts needed for testing \n",
    "    For these techniques: 0Shot, FewShot, COT\n",
    "\n",
    "    We need to decide on which properties are we using..\n",
    "    For now ill be using manual prompts\n",
    "    this is the user input..\n",
    "    \"\"\"\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f842837",
   "metadata": {},
   "source": [
    "# Run models x prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13549bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from openai import OpenAI\n",
    "\n",
    "def runGemini(model,prompt,inputtest):\n",
    "    \n",
    "\n",
    "   # ADD API KEY\n",
    "    client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=prompt),\n",
    "            contents=inputtest\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def runGpt(model,prompt,inputtest):\n",
    "    # ADD API_KEY HERE\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(\n",
    "    )\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        instructions=prompt,\n",
    "        input=inputtest,\n",
    "        store=True,\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "# Sempre manter o mesmo prompt, o que muda é o user input -- que vao ser incorporados as entities e as propriedades pedidas..\n",
    "# Mas como por nequanto vou manter as mesmas propriedades, \n",
    "\n",
    "# TODO: modificar os prompts para pedir que ele siga o que o usuario indicar\n",
    "\n",
    "def getPrompts(promptsPath=\"../prompts/\"):\n",
    "    promptsFiles = os.listdir(promptsPath)\n",
    "    prompts = []\n",
    "\n",
    "    for filename in promptsFiles:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(promptsPath, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                prompts.append(content)\n",
    "\n",
    "    promptsFiles = [x.replace('.txt','') for x in promptsFiles]\n",
    "    return promptsFiles, prompts\n",
    "\n",
    "\n",
    "def getInputs(inputs_path=\"../inputs/\"):\n",
    "        \n",
    "    inputFilesTest = os.listdir(inputs_path)\n",
    "    inputTexts = []\n",
    "\n",
    "    for filenameInput in inputFilesTest:\n",
    "        if filenameInput.endswith(\".txt\"):\n",
    "            file_path = os.path.join(inputs_path, filenameInput)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                inputTexts.append(content)\n",
    "\n",
    "    return inputFilesTest, inputTexts\n",
    "\n",
    "\n",
    "\n",
    "def generateUserInput(props='iata, name, city'):\n",
    "    \"\"\"\n",
    "    Needs to generate all of the possible user inputs (combination of attributes and inputs)\n",
    "    for now, attributes will be the same.. replace [PROPERTIES] and [INPUT]\n",
    "    \"\"\"\n",
    "    \n",
    "    userBasePrompt = \"\"\"\"\n",
    "    These are the attributes extracted from the database for you to enrich the properties:\n",
    "\n",
    "    [INPUT]\n",
    "    \"\"\"\n",
    "\n",
    "    inputFiles, inputTexts = getInputs()\n",
    "\n",
    "    completedUserInput = []\n",
    "\n",
    "    for index, instance in enumerate(inputTexts):\n",
    "        completedUserPrompt = userBasePrompt.replace('[PROPERTIES]', props).replace('[INPUT]', instance)\n",
    "        completedUserInput.append(completedUserPrompt)\n",
    "\n",
    "    inputFiles = [x.replace('.txt','') for x in inputFiles]\n",
    "    return inputFiles,completedUserInput\n",
    "\n",
    "MODELS = ['gemini-2.5-pro']\n",
    "promptsFiles,PROMPTS = getPrompts()\n",
    "inputsFiles,inputUser = generateUserInput()\n",
    "\n",
    "PROMPTS = PROMPTS[:3]\n",
    "\n",
    "\n",
    "\n",
    "# inicialmente vai salvar cada run e cada coisa num diretorio, depois tem um codigo que le todo o diretorio e monta o majority vote e depois \n",
    "# monta as métricas\n",
    "def runTests(n):\n",
    "    for indexInput, inputtest in enumerate(inputUser):\n",
    "        for model in MODELS:\n",
    "            for indexPrompt, prompt in enumerate(PROMPTS):\n",
    "                for run in range(n):\n",
    "                    # N runs for a specific prompt of a specific model for the same input\n",
    "                    if 'gemini' in model:\n",
    "                        response = runGemini(model, prompt,inputtest)\n",
    "                    elif 'gpt' in model:\n",
    "                        response = runGpt(model, prompt, inputtest)\n",
    "                    output_path = \"results_run/\"+f\"{model}_{inputsFiles[indexInput]}_{promptsFiles[indexPrompt]}_{run}.csv\"\n",
    "                    with open(output_path, \"w\", encoding=\"utf-8\") as outputFile:\n",
    "                        outputFile.write(response)\n",
    "\n",
    "runTests(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36071dc",
   "metadata": {},
   "source": [
    "## Majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cdb7cccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gemini-2.5-pro_small_gru_compact_': {'name': 'São Paulo/Guarulhos International Airport', 'city': 'São Paulo'}, 'gemini-2.5-pro_big_gru_few3_': {'iata': 'GRU', 'name': 'São Paulo-Guarulhos International Airport', 'city': 'Guarulhos'}, 'gemini-2.5-pro_big_gru_few5_': {'iata': 'GRU', 'name': 'São Paulo/Guarulhos–Governador André Franco Montoro International Airport', 'city': 'Guarulhos'}, 'gemini-2.5-pro_big_gru_compact_': {'iata': 'GRU', 'name': 'São Paulo/Guarulhos–Governador André Franco Montoro International Airport', 'city': 'São Paulo'}, 'gemini-2.5-pro_bva_few5_': {'iata': 'BVA', 'name': 'Paris–Beauvais Airport', 'city': 'Beauvais'}, 'gemini-2.5-pro_small_gru_few5_': {'name': 'São Paulo/Guarulhos International Airport', 'city': 'São Paulo'}, 'gemini-2.5-pro_bva_few3_': {'iata': 'BVA', 'name': 'Paris–Beauvais Airport', 'city': 'Beauvais'}, 'gemini-2.5-pro_small_gru_few3_': {'name': 'São Paulo/Guarulhos International Airport', 'city': 'Guarulhos'}, 'gemini-2.5-pro_bva_compact_': {'iata': 'BVA', 'name': 'Paris–Beauvais Airport', 'city': 'Beauvais', 'class': 'Location'}}\n"
     ]
    }
   ],
   "source": [
    "# The idea here is the output being model, prompt, results..\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "path = '/results_run'\n",
    "\n",
    "def constructingDataFrame(results):\n",
    "    all_keys = set()\n",
    "    for info in results.values():\n",
    "        all_keys.update(info.keys())\n",
    "\n",
    "    columns = [\"model\"] + sorted(all_keys)\n",
    "    rows = []\n",
    "    for model, info in results.items():\n",
    "        row = {\"model\": model}\n",
    "        row.update(info)\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df.to_csv(\"airports_dynamic.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculateMajorityVote(results):\n",
    "    # Calculates the majority vote for each group\n",
    "    final_results = {}\n",
    "\n",
    "    for test_name, prop_map in results.items():\n",
    "        result = {}\n",
    "\n",
    "        for prop, values in prop_map.items():\n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "            most_common, count = Counter(values).most_common(1)[0]\n",
    "            result[prop] = most_common\n",
    "\n",
    "        final_results[test_name] = result\n",
    "\n",
    "    return final_results \n",
    "\n",
    "def organizeVotes(directory, runs=3):\n",
    "    \"\"\"\n",
    "    Lê todos os arquivos CSV em um diretório.\n",
    "    Agrupa por prefixo (ex.: gemini_prompt1).\n",
    "    Faz majority vote dos valores para cada propriedade.\n",
    "    \"\"\"\n",
    "\n",
    "    grouped_votes = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        test = filename[:-5]\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        with open(file_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            \n",
    "            for row in reader:\n",
    "                prop = row.get(\"property\")\n",
    "                val = row.get(\"value\")\n",
    "\n",
    "                if prop is None or val is None:\n",
    "                    continue\n",
    "\n",
    "                prop = prop.strip()\n",
    "                val = val.strip()\n",
    "                if prop and val:\n",
    "                    grouped_votes[test][prop].append(val)\n",
    "                    \n",
    "    \n",
    "    return calculateMajorityVote(grouped_votes)\n",
    "\n",
    "\n",
    "directory = \"results_run\" \n",
    "results = organizeVotes(directory)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5495a",
   "metadata": {},
   "source": [
    "## Generate ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5efe61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "\n",
    "def getValuesForPropertiesFromNeo4j(entities, properties):\n",
    "    \n",
    "    NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "    NEO4J_USER = \"neo4j\"\n",
    "    NEO4J_PASSWORD = \"password\"\n",
    "\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    def fetch_nodes_and_props(tx, label):\n",
    "        query = f\"\"\"\n",
    "        MATCH (a:Airport)\n",
    "        WHERE a.iata IN {entities}\n",
    "        RETURN {properties}\n",
    "        \"\"\"\n",
    "        # TODO change for icao later\n",
    "        return list(tx.run(query))\n",
    "\n",
    "    with driver.session(database=\"aiportcorrect\") as session:\n",
    "        nodes = session.execute_read(fetch_nodes_and_props, 'Airport')\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    return nodes\n",
    "\n",
    "\n",
    "properties = 'a.iata, a.name, a.city'\n",
    "entities = ['GRU','CDG']\n",
    "results = getValuesForPropertiesFromNeo4j(entities, properties)\n",
    "\n",
    "ground_truth = pd.DataFrame()\n",
    "\n",
    "for index, record in enumerate(results):\n",
    "    row = dict(record)\n",
    "    ground_truth = pd.concat([ground_truth, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Saving to CSV\n",
    "ground_truth.to_csv('ground_truth.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f98a13",
   "metadata": {},
   "source": [
    "## Ranking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4c6cac94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model 'gemini1' has 2 predictions but ground truth has 1 rows.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     60\u001b[39m         final_results[model_name] = results\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m final_results\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m metrics = \u001b[43mevaluate_csv_per_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresult_test1.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mground_truth.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Metrics per model ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model, m \u001b[38;5;129;01min\u001b[39;00m metrics.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mevaluate_csv_per_model\u001b[39m\u001b[34m(pred_csv, truth_csv, tolerance)\u001b[39m\n\u001b[32m     25\u001b[39m truth_model = truth.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pred_model) != \u001b[38;5;28mlen\u001b[39m(truth_model):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     30\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pred_model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m predictions \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut ground truth has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(truth_model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m     )\n\u001b[32m     34\u001b[39m results = {}\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols:\n",
      "\u001b[31mValueError\u001b[39m: Model 'gemini1' has 2 predictions but ground truth has 1 rows."
     ]
    }
   ],
   "source": [
    "# da pra usar o que ja tem pronto das coisas de ML provavelmente.. \n",
    "# gerar por propriedade e depois no geral\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_csv_per_model(pred_csv, truth_csv, tolerance=1e-4):\n",
    "    pred = pd.read_csv(pred_csv)\n",
    "    truth = pd.read_csv(truth_csv)\n",
    "\n",
    "    pred[\"model\"] = pred[\"model\"].astype(str)\n",
    "\n",
    "    pred_no_model = pred.drop(columns=[\"model\"])\n",
    "    truth = truth.drop(columns=[\"id\"])\n",
    "\n",
    "    models = pred[\"model\"].unique()\n",
    "    cols = truth.columns\n",
    "\n",
    "    final_results = {}\n",
    "\n",
    "    for model_name in models:\n",
    "        model_mask = pred[\"model\"] == model_name\n",
    "        \n",
    "        pred_model = pred_no_model[model_mask].reset_index(drop=True)\n",
    "        truth_model = truth.reset_index(drop=True)\n",
    "\n",
    "      \n",
    "        if len(pred_model) != len(truth_model):\n",
    "            raise ValueError(\n",
    "                f\"Model '{model_name}' has {len(pred_model)} predictions \"\n",
    "                f\"but ground truth has {len(truth_model)} rows.\"\n",
    "            )\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for col in cols:\n",
    "            pred_values = pred_model[col].to_numpy()\n",
    "            truth_values = truth_model[col].to_numpy()\n",
    "\n",
    "            if truth[col].dtype == float or truth[col].dtype == int:\n",
    "                correct = (abs(pred_values - truth_values) <= tolerance).astype(int)\n",
    "            else:\n",
    "                pred_norm = pd.Series(pred_values).astype(str).str.strip().str.lower().to_numpy()\n",
    "                truth_norm = pd.Series(truth_values).astype(str).str.strip().str.lower().to_numpy()\n",
    "\n",
    "                correct = (pred_norm == truth_norm).astype(int)\n",
    "\n",
    "            y_true = [1] * len(correct)\n",
    "            y_pred = correct\n",
    "\n",
    "            results[col] = {\n",
    "                \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "                \"precision\": precision_score(y_true, y_pred),\n",
    "                \"recall\": recall_score(y_true, y_pred),\n",
    "                \"f1\": f1_score(y_true, y_pred),\n",
    "                \"num_correct\": correct.sum(),\n",
    "                \"num_total\": len(correct)\n",
    "            }\n",
    "\n",
    "        final_results[model_name] = results\n",
    "\n",
    "    return final_results\n",
    "\n",
    "metrics = evaluate_csv_per_model(\"result_test1.csv\", \"ground_truth.csv\")\n",
    "\n",
    "print(\"\\n=== Metrics per model ===\")\n",
    "for model, m in metrics.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    print(m)\n",
    "\n",
    "# for col, m in metrics.items():\n",
    "#     print(f\"\\nColuna: {col}\")\n",
    "#     for metric_name, value in m.items():\n",
    "#         print(f\"  {metric_name}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "85498857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               model  accuracy_city  accuracy_name  f1_city  \\\n",
      "0  gemini-2.5-pro_small_gru_compact_            0.0            0.0      0.0   \n",
      "1       gemini-2.5-pro_big_gru_few3_            0.0            0.0      0.0   \n",
      "2       gemini-2.5-pro_big_gru_few5_            0.0            0.0      0.0   \n",
      "3    gemini-2.5-pro_big_gru_compact_            0.0            0.0      0.0   \n",
      "4           gemini-2.5-pro_bva_few5_            0.0            0.0      0.0   \n",
      "5     gemini-2.5-pro_small_gru_few5_            0.0            0.0      0.0   \n",
      "6           gemini-2.5-pro_bva_few3_            0.0            0.0      0.0   \n",
      "7     gemini-2.5-pro_small_gru_few3_            0.0            0.0      0.0   \n",
      "8        gemini-2.5-pro_bva_compact_            0.0            0.0      0.0   \n",
      "\n",
      "   f1_name  overall_accuracy  overall_f1_macro  \n",
      "0      0.0               0.0               0.0  \n",
      "1      0.0               0.0               0.0  \n",
      "2      0.0               0.0               0.0  \n",
      "3      0.0               0.0               0.0  \n",
      "4      0.0               0.0               0.0  \n",
      "5      0.0               0.0               0.0  \n",
      "6      0.0               0.0               0.0  \n",
      "7      0.0               0.0               0.0  \n",
      "8      0.0               0.0               0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate_with_sklearn(result_csv, ground_truth_csv, key_column=\"iata\"):\n",
    "\n",
    "    gt = pd.read_csv(ground_truth_csv)\n",
    "    results = pd.read_csv(result_csv)\n",
    "\n",
    "    gt = gt.rename(columns={c: c.split(\".\")[-1] for c in gt.columns})\n",
    "\n",
    "    common_columns = [\n",
    "        c for c in results.columns\n",
    "        if c in gt.columns and c not in [\"model\", key_column]\n",
    "    ]\n",
    "\n",
    "    def norm(x):\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "        return str(x).strip().lower()\n",
    "\n",
    "    gt[key_column] = gt[key_column].apply(norm)\n",
    "    results[key_column] = results[key_column].apply(norm)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for _, row in results.iterrows():\n",
    "        model_name = row[\"model\"]\n",
    "        pred_key = row[key_column]\n",
    "\n",
    "        gt_row = gt[gt[key_column] == pred_key]\n",
    "\n",
    "        if gt_row.empty:\n",
    "            y_true = [\"\"] * len(common_columns)\n",
    "        else:\n",
    "            gt_row = gt_row.iloc[0]\n",
    "            y_true = [norm(gt_row[col]) for col in common_columns]\n",
    "\n",
    "        y_pred = [norm(row[col]) for col in common_columns]\n",
    "\n",
    "        col_acc = {}\n",
    "        col_f1 = {}\n",
    "        for i, col in enumerate(common_columns):\n",
    "            col_acc[f\"accuracy_{col}\"] = accuracy_score([y_true[i]], [y_pred[i]])\n",
    "            col_f1[f\"f1_{col}\"] = f1_score([y_true[i]], [y_pred[i]], average=\"binary\", pos_label=y_true[i])\n",
    "\n",
    "        overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "        overall_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "        metrics.append({\n",
    "            \"model\": model_name,\n",
    "            **col_acc,\n",
    "            **col_f1,\n",
    "            \"overall_accuracy\": overall_accuracy,\n",
    "            \"overall_f1_macro\": overall_f1\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "df_metrics = evaluate_with_sklearn(\"airports_dynamic.csv\", \"ground_truth.csv\")\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6c3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
